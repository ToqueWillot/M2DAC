{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nngraph'\n",
    "require 'torch'\n",
    "require 'math'\n",
    "require 'nn'\n",
    "require 'graph'\n",
    "OMP_NUM_THREADS=1 -- Torch threaded code and BLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat data/input.txt > grep -v '[0-9]' | tr '[:upper:]\\n' '[:lower:] ' | tr -d -c '[:digit:][:lower:]:;.?!)(, ' | tr -s \" \" > data/input.txt\n",
    "\n",
    "magma@locust-ThinkPad-X1-Carbon:~/projects/m2/as/tme6/as|\n",
    "⇒  th prepare_data.lua -txt data/input.txt \n",
    "\n",
    "th prepare_data.lua -txt data/input.txt \n",
    "timer: \t1.0013580322266e-05\t\n",
    "loading text file...\t\n",
    "timer: \t0.00018715858459473\t\n",
    "creating vocabulary mapping...\t\n",
    "timer: \t0.00035405158996582\t\n",
    "putting data into tensor...\t\n",
    "saving two files...\t\n",
    "Done in time (seconds): \t0.0011570453643799\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "rnn_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local inputs = {}\n",
    "table.insert(inputs, nn.Identity()())   -- network input\n",
    "table.insert(inputs, nn.Identity()())   -- c at time t-1\n",
    "table.insert(inputs, nn.Identity()())   -- h at time t-1\n",
    "local input = inputs[1]\n",
    "local prev_c = inputs[2]\n",
    "local prev_h = inputs[3]\n",
    "\n",
    "\n",
    "local i2h = nn.Linear(input_size, 4 * rnn_size)(input)  -- input to hidden\n",
    "local h2h = nn.Linear(rnn_size, 4 * rnn_size)(prev_h)   -- hidden to hidden\n",
    "local preactivations = nn.CAddTable()({i2h, h2h})       -- i2h + h2h\n",
    "\n",
    "\n",
    "-- gates\n",
    "local pre_sigmoid_chunk = nn.Narrow(2, 1, 3 * rnn_size)(preactivations)\n",
    "local all_gates = nn.Sigmoid()(pre_sigmoid_chunk)\n",
    "\n",
    "-- input\n",
    "local in_chunk = nn.Narrow(2, 3 * rnn_size + 1, rnn_size)(preactivations)\n",
    "local in_transform = nn.Tanh()(in_chunk)\n",
    "\n",
    "\n",
    "local in_gate = nn.Narrow(2, 1, rnn_size)(all_gates)\n",
    "local forget_gate = nn.Narrow(2, rnn_size + 1, rnn_size)(all_gates)\n",
    "local out_gate = nn.Narrow(2, 2 * rnn_size + 1, rnn_size)(all_gates)\n",
    "\n",
    "\n",
    "-- previous cell state contribution\n",
    "local c_forget = nn.CMulTable()({forget_gate, prev_c})\n",
    "-- input contribution\n",
    "local c_input = nn.CMulTable()({in_gate, in_transform})\n",
    "-- next cell state\n",
    "local next_c = nn.CAddTable()({\n",
    "  c_forget,\n",
    "  c_input\n",
    "})\n",
    "\n",
    "\n",
    "local c_transform = nn.Tanh()(next_c)\n",
    "local next_h = nn.CMulTable()({out_gate, c_transform})\n",
    "\n",
    "\n",
    "-- module outputs\n",
    "outputs = {}\n",
    "table.insert(outputs, next_c)\n",
    "table.insert(outputs, next_h)\n",
    "\n",
    "-- packs the graph into a convenient module with standard API (:forward(), :backward())\n",
    "m =  nn.gModule(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'nngraph'\n",
    "\n",
    "LSTM = {}\n",
    "\n",
    "function LSTM.create(input_size, rnn_size)\n",
    "  --------------------- input structure ---------------------\n",
    "  local inputs = {}\n",
    "  table.insert(inputs, nn.Identity()())   -- network input\n",
    "  table.insert(inputs, nn.Identity()())   -- c at time t-1\n",
    "  table.insert(inputs, nn.Identity()())   -- h at time t-1\n",
    "  local input = inputs[1]\n",
    "  local prev_c = inputs[2]\n",
    "  local prev_h = inputs[3]\n",
    "\n",
    "  --------------------- preactivations ----------------------\n",
    "  local i2h = nn.Linear(input_size, 4 * rnn_size)(input)   -- input to hidden\n",
    "  local h2h = nn.Linear(rnn_size, 4 * rnn_size)(prev_h)    -- hidden to hidden\n",
    "  local preactivations = nn.CAddTable()({i2h, h2h})        -- i2h + h2h\n",
    "\n",
    "  ------------------ non-linear transforms ------------------\n",
    "  -- gates\n",
    "  local pre_sigmoid_chunk = nn.Narrow(2, 1, 3 * rnn_size)(preactivations)\n",
    "  local all_gates = nn.Sigmoid()(pre_sigmoid_chunk)\n",
    "\n",
    "  -- input\n",
    "  local in_chunk = nn.Narrow(2, 3 * rnn_size + 1, rnn_size)(preactivations)\n",
    "  local in_transform = nn.Tanh()(in_chunk)\n",
    "\n",
    "  ---------------------- gate narrows -----------------------\n",
    "  local in_gate = nn.Narrow(2, 1, rnn_size)(all_gates)\n",
    "  local forget_gate = nn.Narrow(2, rnn_size + 1, rnn_size)(all_gates)\n",
    "  local out_gate = nn.Narrow(2, 2 * rnn_size + 1, rnn_size)(all_gates)\n",
    "\n",
    "  --------------------- next cell state ---------------------\n",
    "  local c_forget = nn.CMulTable()({forget_gate, prev_c})  -- previous cell state contribution\n",
    "  local c_input = nn.CMulTable()({in_gate, in_transform}) -- input contribution\n",
    "  local next_c = nn.CAddTable()({\n",
    "    c_forget,\n",
    "    c_input\n",
    "  })\n",
    "\n",
    "  -------------------- next hidden state --------------------\n",
    "  local c_transform = nn.Tanh()(next_c)\n",
    "  local next_h = nn.CMulTable()({out_gate, c_transform})\n",
    "    \n",
    "    ---[[ adding g for output\n",
    "    local lx = nn.Linear(rnn_size, input_size)(next_h)\n",
    "    local g = nn.SoftMax()(lx)\n",
    "    --]]\n",
    "\n",
    "  --------------------- output structure --------------------\n",
    "  outputs = {}\n",
    "  table.insert(outputs, next_c)\n",
    "  table.insert(outputs, next_h)\n",
    "    ---[[\n",
    "    table.insert(outputs, g)\n",
    "    --]]\n",
    "    \n",
    "    \n",
    "  -- packs the graph into a convenient module with standard API (:forward(), :backward())\n",
    "  return nn.gModule(inputs, outputs)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local input_size = 9\n",
    "local latent_size = 5\n",
    "layer = LSTM.create(input_size,latent_size)\n",
    "nn_output = layer:forward({torch.randn(1,input_size), torch.randn(1,latent_size), torch.randn(1,latent_size)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.1401  0.1003 -0.2103  1.2222  0.2659\n",
       "[torch.DoubleTensor of size 1x5]\n",
       "\n",
       " 0.0491  0.0778 -0.1089  0.7205  0.0457\n",
       "[torch.DoubleTensor of size 1x5]\n",
       "\n",
       " 0.1521  0.1253  0.0605  0.1003  0.0959  0.1637  0.0960  0.1277  0.0786\n",
       "[torch.DoubleTensor of size 1x9]\n",
       "\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nn_output[1])\n",
    "print(nn_output[2])\n",
    "print(nn_output[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t1\t\n",
       "2\t2\t\n",
       "3\t5\t\n",
       "4\t4\t\n",
       "5\t5\t\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab={1,2,5,4,5}\n",
    "for i,v in ipairs(tab) do\n",
    "    print(i,v)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--h = latent\n",
    "--x = donnees\n",
    "function create_g(dim_x, dim_h)\n",
    "    local input_h = nn.Identity()()\n",
    "    local lx = nn.Linear(dim_h, dim_x)(input_h)\n",
    "    local model_graph = nn.SoftMax()(lx)\n",
    "    return nn.gModule({input_h}, {model_graph})\n",
    "end\n",
    "\n",
    "function create_h(dim_x, dim_h)\n",
    "    local input_x = nn.Identity()()\n",
    "    local input_h = nn.Identity()()\n",
    "\n",
    "    local lx = nn.Linear(dim_x, dim_h)(input_x)\n",
    "    local lh = nn.Linear(dim_h, dim_h)(input_h)\n",
    "\n",
    "    local res = nn.CAddTable()({lx, lh})\n",
    "    local model_graph = nn.Tanh()(res)\n",
    "\n",
    "    return nn.gModule({input_h, input_x}, {model_graph})\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = nn.Identity()()\n",
    "x2 = nn.Identity()()\n",
    "a = nn.CMulTable()({x1,x2})\n",
    "aa = nn.CAddTable()({a,x1})\n",
    "m = nn.gModule({x1,x2},{aa})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  3\n",
       " 10\n",
       "[torch.DoubleTensor of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m:forward({torch.Tensor{1,2},torch.Tensor{2,4}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "cannot open <data.t7> in mode r  at /Users/meat/torch/pkg/torch/lib/TH/THDiskFile.c:484\nstack traceback:\n\t[C]: at 0x0ffb6380\n\t[C]: in function 'DiskFile'\n\t/Users/meat/torch/install/share/lua/5.1/torch/File.lua:309: in function 'load'\n\t./util/CharLMMinibatchLoader.lua:15: in function 'create'\n\t[string \"local CharLMMinibatchLoader=require 'util.Cha...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/meat/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/meat/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/meat/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/meat/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/meat/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/meat/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/meat/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/meat/Library/Jupyter/runtime/ker...\"]:1: in main chunk",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "cannot open <data.t7> in mode r  at /Users/meat/torch/pkg/torch/lib/TH/THDiskFile.c:484\nstack traceback:\n\t[C]: at 0x0ffb6380\n\t[C]: in function 'DiskFile'\n\t/Users/meat/torch/install/share/lua/5.1/torch/File.lua:309: in function 'load'\n\t./util/CharLMMinibatchLoader.lua:15: in function 'create'\n\t[string \"local CharLMMinibatchLoader=require 'util.Cha...\"]:4: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/meat/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/meat/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/meat/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/meat/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/meat/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/meat/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/meat/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/meat/Library/Jupyter/runtime/ker...\"]:1: in main chunk"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loading data files...\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local CharLMMinibatchLoader=require 'util.CharLMMinibatchLoader'\n",
    "\n",
    "batch_size = 1; seq_length = 5\n",
    "v=CharLMMinibatchLoader.create(\"data.t7\",\"vocab.t7\",batch_size, seq_length)\n",
    "print(v)\n",
    "print(v.x_batches[1])\n",
    "print(v.y_batches[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to float and onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  y : table: 0x4054f1e8\n",
       "  x : table: 0x40f895f8\n",
       "}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "[string \"#convert bytensors to float...\"]:6: attempt to index global 'v' (a nil value)\nstack traceback:\n\t[string \"#convert bytensors to float...\"]:6: in main chunk\n\t[C]: in function 'xpcall'\n\t/home/magma/bin/torch/install/share/lua/5.1/itorch/main.lua:179: in function </home/magma/bin/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/home/magma/bin/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...magma/bin/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...magma/bin/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...magma/bin/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/home/magma/bin/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x00405ab0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"#convert bytensors to float...\"]:6: attempt to index global 'v' (a nil value)\nstack traceback:\n\t[string \"#convert bytensors to float...\"]:6: in main chunk\n\t[C]: in function 'xpcall'\n\t/home/magma/bin/torch/install/share/lua/5.1/itorch/main.lua:179: in function </home/magma/bin/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/home/magma/bin/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...magma/bin/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...magma/bin/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...magma/bin/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/home/magma/bin/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x00405ab0"
     ]
    }
   ],
   "source": [
    "#convert bytensors to float\n",
    "data = {}\n",
    "data['x'] = {} \n",
    "data['y'] = {}\n",
    "print (data)\n",
    "for i = 1, #v.x_batches do\n",
    "    data['x'][i] = v.x_batches[i]:int()\n",
    "    data['y'][i] = v.y_batches[i]:int()\n",
    "end\n",
    "\n",
    "--get size of dictionary\n",
    "function from_seq_to_vecs (seq, map, size_d)\n",
    "    vecs = {}\n",
    "    for i = 1, seq:size(2) do\n",
    "        t = torch.Tensor(size_d):zero()\n",
    "        t[seq[1][i]] = 1\n",
    "        vecs[i] = t\n",
    "    end\n",
    "    return vecs    \n",
    "end\n",
    "n=0; for k,_ in pairs(v.vocab_mapping) do n=n+1 end;\n",
    "size_d = n\n",
    "for i = 1, #v.x_batches do\n",
    "    data['x'][i] = from_seq_to_vecs(data['x'][i], v.vocab_mapping, size_d)\n",
    "    data['y'][i] = from_seq_to_vecs(data['y'][i], v.vocab_mapping, size_d)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 25\n",
       "  2 : DoubleTensor - size: 25\n",
       "  3 : DoubleTensor - size: 25\n",
       "  4 : DoubleTensor - size: 25\n",
       "  5 : DoubleTensor - size: 25\n",
       "}\n",
       "{\n",
       "  1 : DoubleTensor - size: 25\n",
       "  2 : DoubleTensor - size: 25\n",
       "  3 : DoubleTensor - size: 25\n",
       "  4 : DoubleTensor - size: 25\n",
       "  5 : DoubleTensor - size: 25\n",
       "}\n",
       "{\n",
       "  1 : DoubleTensor - size: 25\n",
       "  2 : DoubleTensor - size: 25\n",
       "  3 : DoubleTensor - size: 25\n",
       "  4 : DoubleTensor - size: 25\n",
       "  5 : DoubleTensor - size: 25\n",
       "}\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print (data['x'][1])\n",
    "\n",
    "print (data['x'][2])\n",
    "\n",
    "print(data['y'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Définition de la fonction g\n",
    "\n",
    "Pour commencer, en consid ́erant un espace latent de dimension N , et un\n",
    "espace d’ ́entr ́ee de dimension n, d ́efinissez sous forme de module (en utilisant\n",
    "nngraph) la fonction g τ sous la forme d’une fonction lin ́eaire suivie d’un softmax.\n",
    "2 : D ́"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--N = espace latent, n = espace d'entrée\n",
    "function create_g(N,n)\n",
    "    local input_x = nn.Identity()()\n",
    "    local lx = nn.Linear(N, n)(input_x)\n",
    "    local model_graph = nn.SoftMax()(lx)\n",
    "    return nn.gModule({input_x}, {model_graph})\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Definition de la fonction h\n",
    "D ́efinissez sous forme de module la fonction h θ de la forme\n",
    "h t = tanh(Θ d h t−1 + Θ i w t−1\n",
    "(2)\n",
    "o`\n",
    "u w t−1 est le vecteur z ́ero, avec une unique valeur 1 correspondant `a l’ ́el ́ement\n",
    "lu par le r ́eseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--N = dim_x, n = dim_h\n",
    "function create_h(dim_x, dim_h)\n",
    "    local input_x = nn.Identity()()\n",
    "    local input_h = nn.Identity()()\n",
    "\n",
    "    local lx = nn.Linear(dim_x, dim_h)(input_x)\n",
    "    local lh = nn.Linear(dim_h, dim_h)(input_h)\n",
    "    \n",
    "    local res = nn.CAddTable()({lx, lh})    \n",
    "    local model_graph = nn.Tanh()(res)\n",
    "    \n",
    "    return nn.gModule({input_x, input_h}, {model_graph})\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Clonage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function clone_many_times(net, T)\n",
    "    local clones = {}\n",
    "\n",
    "    local params, gradParams\n",
    "    if net.parameters then\n",
    "        params, gradParams = net:parameters()\n",
    "        if params == nil then\n",
    "            params = {}\n",
    "        end\n",
    "    end\n",
    "\n",
    "    local paramsNoGrad\n",
    "    if net.parametersNoGrad then\n",
    "        paramsNoGrad = net:parametersNoGrad()\n",
    "    end\n",
    "\n",
    "    local mem = torch.MemoryFile(\"w\"):binary()\n",
    "    mem:writeObject(net)\n",
    "\n",
    "    for t = 1, T do\n",
    "        -- We need to use a new reader for each clone.\n",
    "        -- We don't want to use the pointers to already read objects.\n",
    "        local reader = torch.MemoryFile(mem:storage(), \"r\"):binary()\n",
    "        local clone = reader:readObject()\n",
    "        reader:close()\n",
    "\n",
    "        if net.parameters then\n",
    "            local cloneParams, cloneGradParams = clone:parameters()\n",
    "            local cloneParamsNoGrad\n",
    "            for i = 1, #params do\n",
    "                cloneParams[i]:set(params[i])\n",
    "                cloneGradParams[i]:set(gradParams[i])\n",
    "            end\n",
    "            if paramsNoGrad then\n",
    "                cloneParamsNoGrad = clone:parametersNoGrad()\n",
    "                for i =1,#paramsNoGrad do\n",
    "                    cloneParamsNoGrad[i]:set(paramsNoGrad[i])\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        clones[t] = clone\n",
    "        collectgarbage()\n",
    "    end\n",
    "\n",
    "    mem:close()\n",
    "    return clones\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25\t\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['x'][1][1]:size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25\t\n",
       "10\t\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--model_utils=require 'model_utils'\n",
    "\n",
    "-- creation des modules h et g\n",
    "n= data['x'][1][1]:size(1) --espace d' entrée\n",
    "N= 10--espace latent\n",
    "\n",
    "print (n)\n",
    "print (N)\n",
    "local g = create_g(N, n)\n",
    "local h = create_h(n, N)\n",
    "\n",
    "graph.dot(g.fg, 'g_unit', 'g_unit')\n",
    "graph.dot(h.fg, 'h_unit', 'h_unit')\n",
    "\n",
    "T = 5 -- time steps : longueur de la fenetre\n",
    "\n",
    "local clones_g = clone_many_times(g, T)\n",
    "local clones_h = clone_many_times(h, T)\n",
    "\n",
    "function build_rnn(clones_g, clones_h)\n",
    "    inputs = {nn.Identity()()}\n",
    "    outputs = {}\n",
    "    for t = 1, T do\n",
    "        inputs[t+1] = nn.Identity()() \n",
    "        if (t==1) then\n",
    "            gnode = clones_g[t]({inputs[1], inputs[t+1]})\n",
    "        else\n",
    "            gnode = clones_g[t]({gnode, inputs[t+1]})\n",
    "        end\n",
    "        outputs[t] = clones_h[t](gnode)\n",
    "    end\n",
    "\n",
    "\n",
    "    return nn.gModule(inputs, outputs)\n",
    "end\n",
    "model = build_rnn(clones_g, clones_h)\n",
    "\n",
    "graph.dot(model.fg, 'rnn', 'rnn')\n",
    "\n",
    "--print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--utils\n",
    "\n",
    "--[]\n",
    "--\n",
    "function concat_2tables(table1, table2) \n",
    "    print (table1)\n",
    "    len = 0; for k,_ in pairs(table1) do len=len+1 end;\n",
    "    for key, val in pairs(table2)do\n",
    "        table1[key+len] = val\n",
    "    end\n",
    "    return table1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "[string \"epsilon = 1e-3...\"]:10: ')' expected near '1'",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"epsilon = 1e-3...\"]:10: ')' expected near '1'"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-3\n",
    "loss = nn.ClassNLLCriterion()\n",
    "function train_rnn(model, loss, data, epsilon) \n",
    "    nSeq=0; for k,_ in pairs(data['x']) do nSeq=nSeq+1 end;\n",
    "    print (nSeq)\n",
    "\n",
    "    --h_in = torch.Tensor(1,n):zero()\n",
    "    losses = nn.ParallelCriterion()\n",
    "    for i =1, T do\n",
    "        losses:add(nn.ClassNLLCriterion() 1/T)\n",
    "    end\n",
    "    -- print (T)\n",
    "    for i = 0, nSeq do\n",
    "        losses:add(loss, 1/T)\n",
    "    end \n",
    "    \n",
    "    for i = 1, nSeq do\n",
    "        model:zeroGradParameters()            \n",
    "        h_in = torch.Tensor(N):zero()\n",
    "        \n",
    "        print (\"avant\")\n",
    "        in_seq = concat_2tables({h_in}, data['x'][i])\n",
    "        --print (data[])\n",
    "        print (in_seq)\n",
    "        out = model:forward(in_seq)\n",
    "        --print (out)\n",
    "        --convertir out qui sont des log probas, en probas \n",
    "        \n",
    "        print (\"apres\")\n",
    "        err = losses:forward(out, data['y'][i]) --err ici\n",
    "        delta = losses:backward(out, data['y'][i])\n",
    "        model:backward(in_seq, delta) \n",
    "        model:updateParameters(epsilon)\n",
    "    end\n",
    "end\n",
    "train_rnn(model, loss, data, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"function generate_seq(init_seq, model I)...\"]:1: ')' expected near 'I'",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"function generate_seq(init_seq, model I)...\"]:1: ')' expected near 'I'"
     ]
    }
   ],
   "source": [
    "function generate_seq(init_seq, model I)\n",
    "    res = {}\n",
    "    -- mettre initseq dans res\n",
    "    --faire conversion de init_seq ici\n",
    "    --pour l instant on lui donne un vecteur random \n",
    "    input = initseq -- transformed\n",
    "    for i = 0, I do\n",
    "        -- ajouter vecteur h ici..\n",
    "               \n",
    "        h_in = torch.Tensor(N):zero()\n",
    "        output = model:forward(init_seq)\n",
    "\n",
    "        --puis passer le output en entrée\n",
    "        input = output\n",
    "        res[i] = output[output:size()] -- on prend le dernier element\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
