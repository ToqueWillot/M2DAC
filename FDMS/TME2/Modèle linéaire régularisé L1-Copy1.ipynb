{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDMS TME2\n",
    "Florian Toque & Paul Willot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn import cross_validation\n",
    "from sklearn import base\n",
    "#mnist = fetch_mldata('iris')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('iris')\n",
    "\n",
    "X= mnist.data\n",
    "y= mnist.target\n",
    "\n",
    "for idx,i in enumerate(y):\n",
    "    if (i==2) or (i==3):\n",
    "        y[idx]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = sklearn.datasets.make_classification(n_samples=10000,\n",
    "                                          n_features=30,\n",
    "                                          n_informative=15,\n",
    "                                          n_redundant=0,\n",
    "                                          n_repeated=0,\n",
    "                                          n_classes=2,\n",
    "                                          n_clusters_per_class=2,\n",
    "                                          weights=None,\n",
    "                                          flip_y=0.01,\n",
    "                                          class_sep=1.0,\n",
    "                                          hypercube=True,\n",
    "                                          shift=0.0,\n",
    "                                          scale=1.0,\n",
    "                                          shuffle=True,\n",
    "                                          random_state=None)\n",
    "X= ds[0]\n",
    "y= ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50045699  1.05140495 -1.01375929  0.33525007 -1.46222837  0.14711529\n",
      "  0.09067357 -1.21588674  0.86698472 -0.95093595 -0.65936801  0.70344426\n",
      "  1.60129662  0.86135045 -4.0775631   1.3856939   0.65483106 -0.89254591\n",
      " -0.42821604 -0.94137002 -1.22760844 -2.24213938 -0.04029893  0.4398061\n",
      "  0.62084197 -0.9264954   1.78451019 -0.54460813 -0.26447204 -1.32412621]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# labels: [0,1] -> [-1,1]\n",
    "for idx,i in enumerate(y):\n",
    "    if (i==0):\n",
    "        y[idx]=-1\n",
    "\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Pseudo-code descente de gradient L1\n",
    "\n",
    "procedureGradientL1(θ,X,Y,I;ε)\n",
    "    for it = 1, I do\n",
    "    \n",
    "        for i = 1, n do\n",
    "            idx ← random(l)\n",
    "            θ′ ← θ − ε∇θ L(θ)(idx)\n",
    "            for j=1,n do\n",
    "                if θj′ ∗ θj < 0 then\n",
    "                    θj ← 0\n",
    "                else\n",
    "                    θ j ← θ j′\n",
    "                end if\n",
    "            end for\n",
    "        end for\n",
    "        Afficher L(θ) et accuracy(θ) pour controle\n",
    "        \n",
    "    end for\n",
    "    return θ \n",
    "end procedure\n",
    "\n",
    "L(θ) = 1/l * somme de 1 a l:( yi − fθ(xi))2 + λC(θ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientDescent(base.BaseEstimator):\n",
    "    def __init__(self,theta,lamb,eps):\n",
    "        #t = copy.deepcopy(theta)\n",
    "        #self.theta=t\n",
    "        self.theta=theta\n",
    "        self.eps=eps\n",
    "        self.lamb=lamb\n",
    "        self.tmp=self.theta\n",
    "\n",
    "    def fit(self,X,y,nbIt=1000,printevery=-1):\n",
    "        l=len(X)\n",
    "        xTrans = X.transpose()\n",
    "        \n",
    "        for i in xrange(0,nbIt):\n",
    "            #self.theta=self.tmp\n",
    "            if printevery!=-1 and i%printevery==0:\n",
    "                pass\n",
    "                    \n",
    "            index = np.random.randint(l)\n",
    "            loss = np.dot(X, self.theta) - y\n",
    "            cost = np.sum(loss ** 2) * (1 / l) + (self.lamb*np.linalg.norm(self.theta))\n",
    "            #gradient = np.dot(xTrans,(np.dot(theta,xTrans)-y))+np.sign(theta)*self.lamb\n",
    "            #thetaprime = self.theta - self.eps * gradient\n",
    "            gradient = np.dot(xTrans,(np.dot(theta,xTrans)-y))\n",
    "            thetaprime = self.theta - self.eps * gradient\n",
    "            thetaprime = self.theta - self.eps * np.sign(theta)*self.lamb\n",
    "            #if i%(nbIt/100)==0:\n",
    "            #    thetaprime = self.theta - self.eps * (np.sign(theta)*self.lamb)\n",
    "            #else:\n",
    "            #    thetaprime = self.theta - self.eps * gradient\n",
    "            \n",
    "            for k in xrange(0,len(self.theta)):\n",
    "                self.theta[k] = 0 if thetaprime[k]*self.theta[k]<0 else thetaprime[k]   \n",
    "\n",
    "            self.tmp=self.theta\n",
    "            if printevery!=-1 and i%printevery==0:\n",
    "                    print(\"Iteration %s | Cost: %f | Score: %.03f\" % (str(i).ljust(6), cost,self.score(X,y)))\n",
    "                    print(\"%d features used\"%(self.nb_used_features()))\n",
    "                \n",
    "    def predict(self,x):\n",
    "        #print(\"Product: %f\"%(np.dot(x,self.theta)))\n",
    "        ret=[]\n",
    "        for i in x:\n",
    "            ret.append(1 if np.dot(i,self.theta)>0 else -1)\n",
    "        return ret\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        cpt=0.0\n",
    "        allpred = self.predict(X)\n",
    "        for idx,i in enumerate(allpred):\n",
    "            cpt += 1 if i==y[idx] else 0\n",
    "        #print(cpt,len(X))\n",
    "        return cpt/len(X)\n",
    "    \n",
    "    def nb_used_features(self):\n",
    "        cpt=len(self.tmp)\n",
    "        for ii in self.tmp:\n",
    "            if ii==0:\n",
    "                cpt-=1\n",
    "        return cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GradientDescent(base.BaseEstimator):\n",
    "    def __init__(self,theta,lamb,eps):\n",
    "        self.theta=theta\n",
    "        self.eps=eps\n",
    "        self.lamb=lamb\n",
    "\n",
    "    def fit(self,X,y,nbIt=1000,printevery=-1):\n",
    "        l=len(X)\n",
    "        xTrans = X.transpose()\n",
    "        \n",
    "        for i in xrange(0,nbIt):\n",
    "            index = np.random.randint(l)\n",
    "            loss = np.dot(X, self.theta) - y\n",
    "            #cost = np.sum(loss ** 2) / (2 * l) + (self.lamb*np.linalg.norm(self.theta))\n",
    "            \n",
    "            \n",
    "            cost = np.sum(loss ** 2) * (1 / l) + (self.lamb*np.linalg.norm(self.theta))\n",
    "            #gradient = np.dot(xTrans,(np.dot(theta,xTrans)-y))+np.sign(theta)*self.lamb\n",
    "            #thetaprime = self.theta - self.eps * gradient\n",
    "            \n",
    "            gradient = np.dot(xTrans,(np.dot(self.theta,xTrans)-y))\n",
    "            #thetaprime = self.theta - self.eps * gradient\n",
    "            #thetaprime = self.theta - self.eps * np.sign(theta)*self.lamb\n",
    "            \n",
    "            if i%(nbIt/100)==0:\n",
    "                thetaprime = self.theta - self.eps * (np.sign(theta)*self.lamb)\n",
    "            else:\n",
    "                thetaprime = self.theta - self.eps * gradient\n",
    "            \n",
    "            for k in xrange(0,len(theta)):\n",
    "                theta[k] = 0 if thetaprime[k]*theta[k]<0 else thetaprime[k]\n",
    "\n",
    "            if printevery!=-1 and i%printevery==0:\n",
    "                    print(\"Iteration %s | Cost: %f | Score: %.03f\" % (str(i).ljust(6), cost,self.score(X,y)))\n",
    "                    print(\"%d features used\"%(self.nb_used_features()))\n",
    "                \n",
    "    def predict(self,x):\n",
    "        #print(\"Product: %f\"%(np.dot(x,self.theta)))\n",
    "        ret=[]\n",
    "        for i in x:\n",
    "            ret.append(1 if np.dot(i,self.theta)>0 else -1)\n",
    "        return ret\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        cpt=0.0\n",
    "        allpred = self.predict(X)\n",
    "        for idx,i in enumerate(allpred):\n",
    "            cpt += 1 if i==y[idx] else 0\n",
    "        #print(cpt,len(X))\n",
    "        return cpt/len(X)\n",
    "    \n",
    "    def nb_used_features(self):\n",
    "        cpt=len(self.theta)\n",
    "        for ii in self.theta:\n",
    "            if ii==0:\n",
    "                cpt-=1\n",
    "        return cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#theta = np.zeros(len(X[0]))\n",
    "theta = X[0]\n",
    "lamb=0.1\n",
    "eps=0.0001\n",
    "\n",
    "#gd = SimpleGradientDescent(theta,eps)\n",
    "gd = GradientDescent(theta,lamb,eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0      | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 200    | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 400    | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 600    | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 800    | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 1000   | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 1200   | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 1400   | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 1600   | Cost: nan | Score: 0.500\n",
      "30 features used\n",
      "Iteration 1800   | Cost: nan | Score: 0.500\n",
      "30 features used\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GradientDescent' object has no attribute 'tmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2aff2456bc48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnbIterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnbIterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprintevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnbIterations\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_used_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GradientDescent' object has no attribute 'tmp'"
     ]
    }
   ],
   "source": [
    "nbIterations = 2000\n",
    "gd.fit(X,y,nbIterations,printevery=nbIterations/10)\n",
    "print(gd.tmp)\n",
    "print(gd.nb_used_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gd.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoresGrad = cross_validation.cross_val_score(gd, X, y, cv=5,scoring=\"accuracy\")\n",
    "print(\"Cross validation scores: %s, mean: %.02f\"%(scoresGrad,np.mean(scoresGrad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps=0.00001\n",
    "la = []\n",
    "cross_sc = []\n",
    "used_features = []\n",
    "\n",
    "for lamb in np.arange(0,1.1,0.1):\n",
    "    theta = X[0]\n",
    "    gd = GradientDescent(theta,lamb,eps)\n",
    "    nbIterations = 1000\n",
    "    gd.fit(X,y,nbIterations)\n",
    "    scoresSvm = cross_validation.cross_val_score(gd, X, y, cv=5,scoring=\"accuracy\")\n",
    "    print(\"Lamda: %.02f | Cross val mean: %.02f | Features: %d\"%(lamb,np.mean(scoresSvm),gd.nb_used_features()))\n",
    "    cross_sc.append(np.mean(scoresSvm))\n",
    "    la.append(lamb)\n",
    "    used_features.append(gd.nb_used_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps=0.00001\n",
    "la = []\n",
    "cross_sc = []\n",
    "used_features = []\n",
    "\n",
    "for lamb in np.arange(0,1.1,0.1):\n",
    "    #theta = np.zeros(len(X[0]))\n",
    "    theta = X[0]\n",
    "    lamb=0.4\n",
    "    eps=0.00001\n",
    "\n",
    "    #gd = SimpleGradientDescent(theta,eps)\n",
    "    gd = GradientDescent(theta,lamb,eps)\n",
    "    nbIterations = 2000\n",
    "    gd.fit(X,y,nbIterations,printevery=nbIterations/10)\n",
    "    print(gd.nb_used_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(la, cross_sc, '#6DC433')\n",
    "ax2.plot(la, used_features, '#5AC8ED')\n",
    "\n",
    "ax1.set_xlabel('lambda')\n",
    "ax1.set_ylabel('Cross val score', color='#6DC433')\n",
    "ax2.set_ylabel('Nb features used', color='#5AC8ED')\n",
    "\n",
    "ax1.yaxis.grid(False)\n",
    "ax2.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientDescentL2(base.BaseEstimator):\n",
    "    def __init__(self,theta,lamb,eps):\n",
    "        self.theta=theta\n",
    "        self.eps=eps\n",
    "        self.lamb=lamb\n",
    "\n",
    "    def fit(self,X,y,nbIt=1000,printevery=-1):\n",
    "        l=len(X)\n",
    "        xTrans = X.transpose()\n",
    "        \n",
    "        for i in xrange(0,nbIt):\n",
    "            index = np.random.randint(l)\n",
    "            loss = np.dot(X, self.theta) - y\n",
    "            cost = np.sum(loss ** 2) / (2 * l) + (self.lamb*(np.linalg.norm(-self.theta)**2))\n",
    "            gradient = np.dot(xTrans,(np.dot(theta,xTrans)-y))+np.sign(theta)*self.lamb\n",
    "            thetaprime = self.theta - self.eps * gradient\n",
    "            \n",
    "            for k in xrange(0,len(theta)):\n",
    "                theta[k] = 0 if thetaprime[k]*theta[k]<0 else thetaprime[k]\n",
    "\n",
    "            if printevery!=-1 and i%printevery==0:\n",
    "                    print(\"Iteration %s | Cost: %f\" % (str(i).ljust(6), cost))\n",
    "                \n",
    "    def predict(self,x):\n",
    "        #print(\"Product: %f\"%(np.dot(x,self.theta)))\n",
    "        ret=[]\n",
    "        for i in x:\n",
    "            ret.append(1 if np.dot(i,self.theta)>0 else -1)\n",
    "        return ret\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        cpt=0.0\n",
    "        allpred = self.predict(X)\n",
    "        for idx,i in enumerate(allpred):\n",
    "            cpt += 1 if i==y[idx] else 0\n",
    "        print(cpt,len(X))\n",
    "        return cpt/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = np.zeros(len(X[0]))\n",
    "lamb=0.05\n",
    "eps=0.00001\n",
    "gd = GradientDescentL2(theta,lamb,eps)\n",
    "\n",
    "nbIterations = 20000\n",
    "gd.fit(X,y,nbIterations,printevery=nbIterations/10)\n",
    "\n",
    "print(\"Score: %s\"%gd.score(X,y))\n",
    "\n",
    "scoresSvm = cross_validation.cross_val_score(gd, X, y, cv=5,scoring=\"accuracy\")\n",
    "print(\"Cross validation scores: %s, mean: %.02f\"%(scoresSvm,np.mean(scoresSvm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps=0.00001\n",
    "la = []\n",
    "cross_sc = []\n",
    "\n",
    "for lamb in np.arange(0,12,0.5):\n",
    "    theta = np.zeros(len(X[0]))\n",
    "    gd = GradientDescentL2(theta,lamb,eps)\n",
    "    nbIterations = 5000\n",
    "    gd.fit(X,y,nbIterations)\n",
    "    scoresSvm = cross_validation.cross_val_score(gd, X, y, cv=5,scoring=\"accuracy\")\n",
    "    print(\"Lamda: %.02f, Cross val mean: %.02f\"%(lamb,np.mean(scoresSvm)))\n",
    "    cross_sc.append(np.mean(scoresSvm))\n",
    "    la.append(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(la,cross_sc)\n",
    "plt.ylabel('Cross val score')\n",
    "plt.xlabel('lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
